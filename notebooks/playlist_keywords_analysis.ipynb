{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "playlists = pl.scan_parquet('../processed_data/data_playlist_metadata.parquet')\n",
    "playlist_tracks = pl.scan_parquet('../processed_data/data_playlist_songs.parquet')\n",
    "tracks = pl.scan_parquet('../processed_data/data_song_metadata.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(expr: pl.Expr) -> pl.Expr:\n",
    "    return expr.str.to_lowercase().str.split(' ')\n",
    "\n",
    "\n",
    "def tokenize_unique(expr: pl.Expr) -> pl.Expr:\n",
    "    return tokenize(expr)\\\n",
    "        .list.filter(pl.element().ne(''))\\\n",
    "        .list.unique(maintain_order=True)\n",
    "\n",
    "\n",
    "def tokenize_filtered(expr: pl.Expr) -> pl.Expr:\n",
    "    return (\n",
    "        tokenize_unique(expr)\n",
    "        # Filter our years & BPM ranges\n",
    "        .list.filter(~pl.element().str.contains(\"^([0-9]+|[0-9]+-[0-9]+)$\"))\n",
    "        # Filter out stuff consisting only of non-letters\n",
    "        .list.filter(pl.element().str.contains(\"[[:alpha:]]\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playlist statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_tokenized = playlists.select(\n",
    "    pl.col('playlist.id'),\n",
    "    pl.col('playlist.name'),\n",
    "    pl.col('playlist.name').pipe(tokenize_filtered).alias('unique_terms'),\n",
    ")\n",
    "\n",
    "exploded_playlists_tokenized = playlists_tokenized\\\n",
    "    .explode('unique_terms')\\\n",
    "    .rename({'unique_terms': 'term'})\n",
    "\n",
    "tokens = exploded_playlists_tokenized\\\n",
    "    .group_by('term')\\\n",
    "    .agg(pl.col('term').count().alias('playlist_count'),\n",
    "         pl.col('playlist.name').head(20))\\\n",
    "    .sort('playlist_count', descending=True)\n",
    "\n",
    "tokens.filter(pl.col('playlist_count').ge(100)).collect(engine='streaming')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
