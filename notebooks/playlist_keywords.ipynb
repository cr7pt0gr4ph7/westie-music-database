{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "show_intermediate_results = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "playlists = pl.scan_parquet('../processed_data/data_playlist_metadata.parquet')\n",
    "playlist_tracks = pl.scan_parquet('../processed_data/data_playlist_songs.parquet')\n",
    "tracks = pl.scan_parquet('../processed_data/data_song_metadata.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists.collect() if show_intermediate_results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_tracks.collect() if show_intermediate_results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.collect()  if show_intermediate_results else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(expr: pl.Expr) -> pl.Expr:\n",
    "    return expr.str.to_lowercase().str.split(' ')\n",
    "\n",
    "def tokenize_unique(expr: pl.Expr) -> pl.Expr:\n",
    "    return tokenize(expr)\\\n",
    "        .list.filter(pl.element().ne(''))\\\n",
    "        .list.unique(maintain_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Tokenize the playlist names by splitting on whitespaces.\n",
    "\n",
    "We currently turn every word into its own separate keyword term.\n",
    "As a later optimization, it might make sense to treat words most often\n",
    "occuring together (e.g. `late night`) to make the output more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_tokenized = playlists.select(\n",
    "    pl.col('playlist.id'),\n",
    "    pl.col('playlist.name'),\n",
    "    pl.col('playlist.name').pipe(tokenize_unique).alias('unique_terms'),\n",
    ")\n",
    "\n",
    "playlists_tokenized.collect(engine='streaming') if show_intermediate_results else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Aggregate over playlist terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_playlists_tokenized = playlists_tokenized\\\n",
    "    .explode('unique_terms')\\\n",
    "    .rename({'unique_terms': 'term'})\n",
    "\n",
    "exploded_playlists_tokenized.limit(100).collect(engine='streaming') if show_intermediate_results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = exploded_playlists_tokenized\\\n",
    "    .group_by('term')\\\n",
    "    .agg(pl.col('term').count().alias('playlist_count'))\\\n",
    "    .sort('playlist_count', descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review query plan for potential performance/memory problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.show_graph(plan_stage='physical', engine='streaming', optimized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.filter(pl.col('playlist_count').ge(100)).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "# tokens.filter(pl.col('playlist_count').ge(20)).sink_csv('playlist_keywords.csv', engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following discoveries where made when manually reviewing the CSV data:\n",
    "\n",
    "- Also split on & remove common punctuation (`(`, `)`, `[`, `]`,`:`, `#` etc.)\n",
    "- Remove certain common words that do not provide any information:\n",
    "  - on\n",
    "  - by\n",
    "  - with\n",
    "  - at\n",
    "  - and\n",
    "  - a\n",
    "  - I\n",
    "  - ...\n",
    "- Unify `90's`/`90s` etc.\n",
    "- Unify `bday`/`birthday`/`b-day` etc.\n",
    "- Check correlations between consecutive words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword <=> Song correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore tokens that appear only a few types to reduce the size of the `join`/`group_by`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens\\\n",
    "    .filter(pl.col('playlist_count').le(5))\\\n",
    "    .group_by(pl.col('playlist_count').alias('max_playlist_count'))\\\n",
    "    .agg(pl.col('playlist_count').count().alias('num_terms'))\\\n",
    "    .sort('max_playlist_count')\\\n",
    "    .collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_tokens = tokens\\\n",
    "    .filter(pl.col('playlist_count').ge(5))\\\n",
    "    .collect(engine='streaming')\n",
    "\n",
    "track_keywords = playlist_tracks\\\n",
    "    .join(exploded_playlists_tokenized.join(relevant_tokens.lazy(), how='semi', on='term'), how='inner', on='playlist.id')\\\n",
    "    .group_by('track.id', 'term')\\\n",
    "    .agg(pl.col('term').count().alias('playlist_count'))\n",
    "\n",
    "track_keywords.limit(50).collect(engine='streaming') if show_intermediate_results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = track_keywords\\\n",
    "    .sort('track.id', 'playlist_count', 'term')\\\n",
    "    .group_by('track.id')\\\n",
    "    .agg(pl.col('term').sort_by('playlist_count', descending=True).head(30))\n",
    "\n",
    "q.show_graph(plan_stage='physical', engine='streaming', optimized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = track_keywords\\\n",
    "#     .sort('track.id', 'playlist_count', 'term')\\\n",
    "#     .group_by('track.id')\\\n",
    "#     .agg(pl.col('term').first())\n",
    "\n",
    "q = track_keywords.count()\n",
    "\n",
    "# q.show_graph(plan_stage='physical', engine='streaming', optimized=True)\n",
    "\n",
    "q.collect() if show_intermediate_results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = 'temp_track_keywords.parquet'\n",
    "track_keywords.sink_parquet(temp_file)\n",
    "track_keywords = pl.scan_parquet(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = 'temp_track_keywords_by_track_id.parquet'\n",
    "track_keywords_by_track_id = track_keywords.sort('track.id')\n",
    "track_keywords_by_track_id.sink_parquet(temp_file)\n",
    "track_keywords_by_track_id = pl.scan_parquet(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_keywords_grouped_by_track_id = track_keywords_by_track_id\\\n",
    "    .join(tracks.slice(20, 20).select('track.id').cache(), how='semi', on='track.id')\\\n",
    "    .group_by('track.id')\\\n",
    "    .agg(pl.col('term').sort_by('playlist_count', descending=True).head(20),\n",
    "         pl.col('playlist_count').sort(descending=True).head(20).alias('playlist_counts'),\n",
    "         pl.col('playlist_count').sort(descending=True).head(20).sum())\\\n",
    "    .join(tracks.select('track.id', 'track.name', 'track.artists'), how='inner', on='track.id')\n",
    "\n",
    "# track_keywords_grouped_by_track_id.show_graph(plan_stage='physical', engine='streaming', optimized=True)\n",
    "track_keywords_grouped_by_track_id.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `group_by` to aggregate a column into a list is currently not supported by Polars' `streaming` engine.\n",
    "To avoid crashing with an OOM, we sequentially process batches of `track.id`s instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def process_track_keywords_batch(tracks_batch: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    return track_keywords_by_track_id\\\n",
    "        .join(tracks_batch, how='semi', on='track.id')\\\n",
    "        .group_by('track.id')\\\n",
    "        .agg(pl.col('term').sort_by('playlist_count', descending=True).head(20),\n",
    "             pl.col('playlist_count').sort(descending=True).head(20).alias('playlist_counts'),\n",
    "             pl.col('playlist_count').sort(descending=True).head(20).sum())\\\n",
    "        .join(tracks_batch.select('track.id', 'track.name', 'track.artists'), how='inner', on='track.id')\n",
    "\n",
    "\n",
    "def process_track_keywords_in_batches():\n",
    "    row_count = tracks.select(pl.len()).collect().item()\n",
    "    batch_size = 5000  # Higher batch sizes are faster but have a righer OOM risk\n",
    "    batch_count = int(math.ceil(row_count / batch_size))\n",
    "\n",
    "    print(f\"Processing {row_count:,} tracks in {batch_count:,} batches of {batch_size:,} items...\")\n",
    "\n",
    "    for batch_index in range(0, batch_count):\n",
    "        batch_start = batch_index * batch_size\n",
    "        print(f\"Processing batch {batch_index:,}/{batch_count:,}\")\n",
    "        batch_result = process_track_keywords_batch(tracks.slice(batch_start, batch_size))\\\n",
    "            .sort('track.id')\n",
    "        batch_result.sink_parquet(f'temp_batch_{batch_index}.parquet')\n",
    "\n",
    "    print(\"Merging batches...\")\n",
    "\n",
    "    merged: pl.LazyFrame | None = None\n",
    "    for batch_index in range(0, batch_count):\n",
    "        batch_data = pl.scan_parquet(f'temp_batch_{batch_index}.parquet')\n",
    "        merged = (batch_data if merged is None else\n",
    "                  merged.merge_sorted(batch_data, 'track.id'))\n",
    "\n",
    "    merged.sink_parquet('temp_keywords_by_track.parquet')\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "process_track_keywords_in_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_by_track = pl.scan_parquet('temp_keywords_by_track.parquet')\n",
    "\n",
    "keywords_by_track.limit(1000).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
