{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "show_intermediate_results = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "playlists = pl.scan_parquet('../processed_data/data_playlist_metadata.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playlist Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists.collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Tokenize the playlist names by splitting on whitespaces.\n",
    "\n",
    "We currently turn every word into its own separate keyword term.\n",
    "As a later optimization, it might make sense to treat words most often\n",
    "occuring together (e.g. `late night`) to make the output more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_tokenized = playlists.select(\n",
    "    pl.col('playlist.id'),\n",
    "    pl.col('playlist.name'),\n",
    "    pl.col('playlist.name').str.to_lowercase().str.split(' ')\n",
    "    .list.filter(pl.element().ne(''))\n",
    "    .list.unique(maintain_order=True).alias('unique_terms'),\n",
    ")\n",
    "\n",
    "playlists_tokenized.collect(engine='streaming') if show_intermediate_results else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Aggregate over playlist terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_playlists_tokenized = playlists_tokenized\\\n",
    "    .explode('unique_terms')\\\n",
    "    .rename({'unique_terms': 'term'})\n",
    "\n",
    "exploded_playlists_tokenized.limit(100).collect(engine='streaming') if show_intermediate_results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = exploded_playlists_tokenized\\\n",
    "    .group_by('term')\\\n",
    "    .agg(pl.col('term').count().alias('playlist_count'))\\\n",
    "    .sort('playlist_count', descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review query plan for potential performance/memory problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.show_graph(plan_stage='physical', engine='streaming', optimized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.filter(pl.col('playlist_count').ge(100)).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "# tokens.filter(pl.col('playlist_count').ge(20)).sink_csv('playlist_keywords.csv', engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following discoveries where made when manually reviewing the CSV data:\n",
    "\n",
    "- Also split on & remove common punctuation (`(`, `)`, `[`, `]`,`:`, `#` etc.)\n",
    "- Remove certain common words that do not provide any information:\n",
    "  - on\n",
    "  - by\n",
    "  - with\n",
    "  - at\n",
    "  - and\n",
    "  - a\n",
    "  - I\n",
    "  - ...\n",
    "- Unify `90's`/`90s` etc.\n",
    "- Unify `bday`/`birthday`/`b-day` etc.\n",
    "- Check correlations between consecutive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
